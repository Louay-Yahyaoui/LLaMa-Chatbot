{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9273841,"sourceType":"datasetVersion","datasetId":5499072}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/loubl00m/llama-finetuned?scriptVersionId=274304212\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Loading modules","metadata":{}},{"cell_type":"code","source":"# Install the necessary libraries\n!pip install -U transformers peft bitsandbytes torch==2.4.0","metadata":{"_uuid":"110707a2-27ec-4e7f-b511-6577247a4aba","_cell_guid":"4d28621d-3bda-4dcd-804a-d7a7cf0b1707","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-07T16:51:29.030176Z","iopub.execute_input":"2025-11-07T16:51:29.030854Z","iopub.status.idle":"2025-11-07T16:54:00.54818Z","shell.execute_reply.started":"2025-11-07T16:51:29.030815Z","shell.execute_reply":"2025-11-07T16:54:00.547288Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.42.3)\nCollecting transformers\n  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting peft\n  Downloading peft-0.17.1-py3-none-any.whl.metadata (14 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nCollecting torch==2.4.0\n  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0) (3.13.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0) (2024.5.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0)\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==3.0.0 (from torch==2.4.0)\n  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0)\n  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\nCollecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nRequirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.32.1)\nCollecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.4.0) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.4.0) (1.3.0)\nDownloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading peft-0.17.1-py3-none-any.whl (504 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m504.9/504.9 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, hf-xet, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, tokenizers, nvidia-cusolver-cu12, transformers, torch, bitsandbytes, peft\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.23.4\n    Uninstalling huggingface-hub-0.23.4:\n      Successfully uninstalled huggingface-hub-0.23.4\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.42.3\n    Uninstalling transformers-4.42.3:\n      Successfully uninstalled transformers-4.42.3\n  Attempting uninstall: torch\n    Found existing installation: torch 2.1.2\n    Uninstalling torch-2.1.2:\n      Successfully uninstalled torch-2.1.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.15 requires torch<2.4,>=1.10, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bitsandbytes-0.48.2 hf-xet-1.2.0 huggingface-hub-0.36.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 peft-0.17.1 tokenizers-0.22.1 torch-2.4.0 transformers-4.57.1 triton-3.0.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom peft import PeftModel\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n)\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\nfrom datasets import load_dataset","metadata":{"_uuid":"5032a9e5-ca69-40f3-8260-64967ed07954","_cell_guid":"e8185547-215b-4643-837b-acf79c6a5225","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-07T16:54:00.550322Z","iopub.execute_input":"2025-11-07T16:54:00.550693Z","iopub.status.idle":"2025-11-07T16:54:17.749285Z","shell.execute_reply.started":"2025-11-07T16:54:00.550661Z","shell.execute_reply":"2025-11-07T16:54:17.748604Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n  warn(\n2025-11-07 16:54:07.873439: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-11-07 16:54:07.873581: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-11-07 16:54:08.005187: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Get the Weights & Biases keys from env and login\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"hf_token\")\nlogin(token = hf_token)","metadata":{"execution":{"iopub.status.busy":"2025-11-07T16:54:17.750396Z","iopub.execute_input":"2025-11-07T16:54:17.751391Z","iopub.status.idle":"2025-11-07T16:54:17.975771Z","shell.execute_reply.started":"2025-11-07T16:54:17.751354Z","shell.execute_reply":"2025-11-07T16:54:17.975017Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Loading model from hugging face","metadata":{}},{"cell_type":"code","source":"torch_dtype = torch.float16\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Quantization config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit = True,\n    bnb_4bit_quant_type = \"nf4\",\n    bnb_4bit_compute_dtype = torch_dtype,\n    bnb_4bit_use_double_quant = True,\n)\n\n# Base model and Lora model paths.\nbase_path = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\nmodel_path = \"LouayYahyaoui/llama-edu\"\n\nbase_model = AutoModelForCausalLM.from_pretrained(base_path, quantization_config = bnb_config, \\\n                                                  device_map = device)\nmodel = PeftModel.from_pretrained(base_model, model_path)\ntokenizer = AutoTokenizer.from_pretrained(model_path)","metadata":{"_uuid":"7d7b33e3-33f7-4e0e-8649-30e499845edf","_cell_guid":"5da35bcf-dd00-420d-9bbb-16b3c9997ca3","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-07T16:54:17.978021Z","iopub.execute_input":"2025-11-07T16:54:17.978661Z","iopub.status.idle":"2025-11-07T16:56:03.631829Z","shell.execute_reply.started":"2025-11-07T16:54:17.978609Z","shell.execute_reply":"2025-11-07T16:56:03.631041Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c622f51a33f48498dec085611aa0a81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a52b720867724142a4ba6b90ffdbef78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dbba9f7aae54a40936c70a402369cb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb3904be23b941b0911675244f7b3c7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f341a2186bdf48df965a09e3f07c8869"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2cdddb37ec9453ab70cc834cc9b8a64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ff202bc976c4e9297ac345072670fa6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"334cb95bf9624d6abfcdaea33f148e15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4759e0f4b2934df086cc3c10f7a47611"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/740 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"485f52cc79d344f489a82cb1aca43381"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ca99d766e844b9bbf2f7ccc0911b0d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0f826430a16423ca55ee36806c6f6d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"176d456c885d46eb99e375836b49bbfe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/439 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cd8f4cf9421463898b1a490909aba8c"}},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"# Interacting with the model","metadata":{}},{"cell_type":"code","source":"def answer(prompt):\n    if isinstance(prompt, str):\n        prompt = [{'role': 'system',\n        # Replace the system prompt here. Below is a simple example.\n        'content': 'You are an educational assistant that helps university students understand their courses.'},\n         {'role': 'user',\n        'content': prompt}]\n    elif isinstance(prompt, list):\n        prompt = prompt[:2]\n    elif isinstance(prompt, dict):\n        prompt = prompt[\"messages\"][:2]\n    \n    end_header = \"<|end_header_id|>\"\n        \n    input_ids = tokenizer.apply_chat_template(prompt, tokenize = True, \\\n                                add_generation_prompt = True, return_tensors=\"pt\")\n    \n    \n    attention_mask = torch.ones_like(input_ids)\n    pad_token_id = tokenizer.pad_token_id \n\n    input_ids = input_ids.to(device)\n    out = model.generate(input_ids, max_new_tokens = 1024, do_sample = True, temperature = 0.6, \\\n    attention_mask = attention_mask, pad_token_id = pad_token_id)\n    \n    response = tokenizer.decode(out[0, input_ids.shape[-1]:], skip_special_tokens=True).strip()\n    \n    return response","metadata":{"_uuid":"68787a57-5c08-4647-bf89-a9405c026da1","_cell_guid":"d3a53861-f3b9-4748-81bf-2a3bd23a94be","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-07T16:56:03.632876Z","iopub.execute_input":"2025-11-07T16:56:03.633147Z","iopub.status.idle":"2025-11-07T16:56:03.639483Z","shell.execute_reply.started":"2025-11-07T16:56:03.633125Z","shell.execute_reply":"2025-11-07T16:56:03.638661Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"ILOS = \"\"\"CLO1.Demonstrate an understanding of computer organization and the functions of the main components (CPU, memory, I/O devices) of a computer.\nCLO2.Map between various machine representations and perform basic computer arithmetic operations as implemented in digital computer hardware, including those on signed and unsigned numbers and floating point types.\nCLO3.Demonstrate an understanding of computer arithmetic operations, including addition, subtraction, multiplication, division, and floating-point arithmetic.\nCLO4.Demonstrate an understanding of modern processors, including logic design conventions, datapath construction, pipelining, and data/control hazards.\nCLO5.Demonstrate an understanding of memory hierarchies and analyze their performance. \tExam, Quiz, Assignment.\t2\nCLO6.Demonstrate an understanding of instruction set architecture, including the differences between RISC and CISC architectures, addressing modes, and assembly language programming basics.\nCLO7.Demonstrate an understanding of  the CPU datapath, CPU components and their functions, the instruction execution cycle, the control unit, and CPU performance.\"\"\"","metadata":{"execution":{"iopub.status.busy":"2025-11-07T16:56:03.640855Z","iopub.execute_input":"2025-11-07T16:56:03.641162Z","iopub.status.idle":"2025-11-07T16:56:03.65088Z","shell.execute_reply.started":"2025-11-07T16:56:03.641135Z","shell.execute_reply":"2025-11-07T16:56:03.649971Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import re\nilos = re.split(r\"CLO\\d+.\", ILOS)[1:]\nilos","metadata":{"execution":{"iopub.status.busy":"2025-11-07T16:58:49.706174Z","iopub.execute_input":"2025-11-07T16:58:49.706504Z","iopub.status.idle":"2025-11-07T16:58:49.713382Z","shell.execute_reply.started":"2025-11-07T16:58:49.706482Z","shell.execute_reply":"2025-11-07T16:58:49.712492Z"},"trusted":true},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"['Demonstrate an understanding of computer organization and the functions of the main components (CPU, memory, I/O devices) of a computer.\\n',\n 'Map between various machine representations and perform basic computer arithmetic operations as implemented in digital computer hardware, including those on signed and unsigned numbers and floating point types.\\n',\n 'Demonstrate an understanding of computer arithmetic operations, including addition, subtraction, multiplication, division, and floating-point arithmetic.\\n',\n 'Demonstrate an understanding of modern processors, including logic design conventions, datapath construction, pipelining, and data/control hazards.\\n',\n 'Demonstrate an understanding of memory hierarchies and analyze their performance. \\tExam, Quiz, Assignment.\\t2\\n',\n 'Demonstrate an understanding of instruction set architecture, including the differences between RISC and CISC architectures, addressing modes, and assembly language programming basics.\\n',\n 'Demonstrate an understanding of  the CPU datapath, CPU components and their functions, the instruction execution cycle, the control unit, and CPU performance.']"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Change user prompt here\nfor ilo in ilos:\n    prompt = \"I'm struggling to \" + ilo.strip() + \" Provide some help.\"\n    print(f\"User prompt: {prompt}\\n\\nAssistant: {answer(prompt)}\\n\")","metadata":{"_uuid":"144c7ccb-9887-4aa2-88af-583663987a9b","_cell_guid":"a4363766-34a9-4a23-b563-e9a5733dc229","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-11-07T16:58:49.714549Z","iopub.execute_input":"2025-11-07T16:58:49.714833Z"},"trusted":true},"outputs":[{"name":"stdout","text":"User prompt: I'm struggling to Demonstrate an understanding of computer organization and the functions of the main components (CPU, memory, I/O devices) of a computer. Provide some help.\n\nAssistant: Computer organization refers to the way a computer's hardware components work together to execute instructions. The CPU (Central Processing Unit) executes instructions, memory stores data and instructions, and I/O devices interact with the user. Understanding these components helps in designing and troubleshooting computer systems. For example, the CPU fetches instructions, decodes them, and performs operations, while memory provides storage for data and instructions. I/O devices handle user input and output. Each component plays a crucial role in the computer's operation. Consider studying computer architecture and design to deepen your understanding.\n\nUser prompt: I'm struggling to Map between various machine representations and perform basic computer arithmetic operations as implemented in digital computer hardware, including those on signed and unsigned numbers and floating point types. Provide some help.\n\nAssistant: Understanding the representation of numbers in binary and performing arithmetic operations is crucial in computer science. The mapping involves converting numbers to binary and understanding how signed and unsigned numbers work. For example, the binary representation of a number can be converted to decimal using the positional value of each bit. Signed numbers use a sign bit to indicate whether the number is positive or negative. Arithmetic operations involve performing bitwise operations on the binary representation of numbers. Practice with examples can help solidify understanding.\n\nUser prompt: I'm struggling to Demonstrate an understanding of computer arithmetic operations, including addition, subtraction, multiplication, division, and floating-point arithmetic. Provide some help.\n\nAssistant: Computer arithmetic operations include addition, subtraction, multiplication, division, and floating-point arithmetic. Addition and subtraction involve basic numerical operations, while multiplication and division deal with scaling and scaling inversely. Floating-point arithmetic handles decimal numbers with precision. Practice using these operations in programming will help you understand them better. For example, in Python, you can use the built-in functions `+`, `-`, `*`, `/` for basic arithmetic and `float()` or `decimal` module for floating-point arithmetic. Focus on understanding how these operations are implemented in hardware and software to improve your skills. Consider working on projects that involve numerical computations to solidify your understanding. You can also explore libraries like NumPy for efficient numerical computations. If you have specific questions or need further clarification, feel free to ask.\n\n","output_type":"stream"}],"execution_count":null}]}