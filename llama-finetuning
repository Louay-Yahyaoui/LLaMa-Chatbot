{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9273841,"sourceType":"datasetVersion","datasetId":5499072}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/loubl00m/llama-chatbot?scriptVersionId=274295511\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Loading modules","metadata":{}},{"cell_type":"code","source":"# Install the necessary libraries\n!pip install -U torch==2.4.0 transformers==4.46.2 datasets==3.1.0 accelerate==1.1.1 \\\n    peft==0.13.2 trl==0.12.1 bitsandbytes==0.46.1\n!pip install --no-cache-dir wandb==0.17.9","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-11-07T16:13:41.775446Z","iopub.execute_input":"2025-11-07T16:13:41.77568Z","iopub.status.idle":"2025-11-07T16:16:47.991234Z","shell.execute_reply.started":"2025-11-07T16:13:41.775649Z","shell.execute_reply":"2025-11-07T16:16:47.990095Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting torch==2.4.0\n  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\nCollecting transformers==4.46.2\n  Downloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting datasets==3.1.0\n  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\nCollecting accelerate==1.1.1\n  Downloading accelerate-1.1.1-py3-none-any.whl.metadata (19 kB)\nCollecting peft==0.13.2\n  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\nCollecting trl==0.12.1\n  Downloading trl-0.12.1-py3-none-any.whl.metadata (10 kB)\nCollecting bitsandbytes==0.46.1\n  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0) (3.13.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.4.0) (2024.5.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0)\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==3.0.0 (from torch==2.4.0)\n  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.2) (0.23.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.2) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.2) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.2) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.2) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.2) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.2) (0.4.3)\nCollecting tokenizers<0.21,>=0.20 (from transformers==4.46.2)\n  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.46.2) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==3.1.0) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==3.1.0) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==3.1.0) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==3.1.0) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets==3.1.0) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==3.1.0) (3.9.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==1.1.1) (5.9.3)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from trl==0.12.1) (13.7.0)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0)\n  Downloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==3.1.0) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==3.1.0) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==3.1.0) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==3.1.0) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==3.1.0) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==3.1.0) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.46.2) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.46.2) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.46.2) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.46.2) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.46.2) (2024.7.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.4.0) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==3.1.0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==3.1.0) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==3.1.0) (2023.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl==0.12.1) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl==0.12.1) (2.17.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.4.0) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl==0.12.1) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==3.1.0) (1.16.0)\nDownloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading accelerate-1.1.1-py3-none-any.whl (333 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.2/333.2 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading trl-0.12.1-py3-none-any.whl (310 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.9/310.9 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tokenizers, nvidia-cusolver-cu12, transformers, torch, datasets, bitsandbytes, accelerate, trl, peft\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.42.3\n    Uninstalling transformers-4.42.3:\n      Successfully uninstalled transformers-4.42.3\n  Attempting uninstall: torch\n    Found existing installation: torch 2.1.2\n    Uninstalling torch-2.1.2:\n      Successfully uninstalled torch-2.1.2\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.20.0\n    Uninstalling datasets-2.20.0:\n      Successfully uninstalled datasets-2.20.0\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.32.1\n    Uninstalling accelerate-0.32.1:\n      Successfully uninstalled accelerate-0.32.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.15 requires torch<2.4,>=1.10, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-1.1.1 bitsandbytes-0.46.1 datasets-3.1.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 peft-0.13.2 tokenizers-0.20.3 torch-2.4.0 transformers-4.46.2 triton-3.0.0 trl-0.12.1\nCollecting wandb==0.17.9\n  Downloading wandb-0.17.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb==0.17.9) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.17.9) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.17.9) (3.1.41)\nRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb==0.17.9) (3.11.0)\nRequirement already satisfied: protobuf!=4.21.0,<6,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.17.9) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.17.9) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from wandb==0.17.9) (6.0.1)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.17.9) (2.32.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.17.9) (2.8.0)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb==0.17.9) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb==0.17.9) (69.0.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb==0.17.9) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb==0.17.9) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb==0.17.9) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb==0.17.9) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb==0.17.9) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb==0.17.9) (2024.7.4)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb==0.17.9) (5.0.1)\nDownloading wandb-0.17.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m138.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: wandb\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.17.4\n    Uninstalling wandb-0.17.4:\n      Successfully uninstalled wandb-0.17.4\nSuccessfully installed wandb-0.17.9\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n)\nfrom huggingface_hub import login, Repository, create_repo\nfrom trl import SFTConfig, SFTTrainer\nfrom kaggle_secrets import UserSecretsClient\nimport wandb","metadata":{"execution":{"iopub.status.busy":"2025-11-07T16:16:47.993634Z","iopub.execute_input":"2025-11-07T16:16:47.993952Z","iopub.status.idle":"2025-11-07T16:17:05.193093Z","shell.execute_reply.started":"2025-11-07T16:16:47.993927Z","shell.execute_reply":"2025-11-07T16:17:05.192318Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n  warn(\n2025-11-07 16:16:54.541977: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-11-07 16:16:54.542097: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-11-07 16:16:54.677024: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Get the Weights & Biases keys from env and login\nuser_secrets = UserSecretsClient()\nwb_key = user_secrets.get_secret(\"wandb_key\")\nwandb.login(key = wb_key)","metadata":{"execution":{"iopub.status.busy":"2025-11-07T16:17:05.194281Z","iopub.execute_input":"2025-11-07T16:17:05.194925Z","iopub.status.idle":"2025-11-07T16:17:08.281455Z","shell.execute_reply.started":"2025-11-07T16:17:05.19489Z","shell.execute_reply":"2025-11-07T16:17:08.280649Z"},"trusted":true},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Get my hugging face token for llama\nhf_token = user_secrets.get_secret(\"hf_token\")\nlogin(token = hf_token, add_to_git_credential=True)","metadata":{"execution":{"iopub.status.busy":"2025-11-07T16:17:08.282479Z","iopub.execute_input":"2025-11-07T16:17:08.283112Z","iopub.status.idle":"2025-11-07T16:17:08.808524Z","shell.execute_reply.started":"2025-11-07T16:17:08.283088Z","shell.execute_reply":"2025-11-07T16:17:08.807683Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Token is valid (permission: write).\n\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\nYou might have to re-authenticate when pushing to the Hugging Face Hub.\nRun the following command in your terminal in case you want to set the 'store' credential helper as default.\n\ngit config --global credential.helper store\n\nRead https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\nToken has not been saved to git credential helper.\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Loading the model","metadata":{}},{"cell_type":"code","source":"# Choosing the attention implementation according to the version of the GPU if it exists.\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nif torch.cuda.get_device_capability()[0] >= 8:\n    !pip install -qqq flash-attn\n    attn_implementation = \"flash_attention_2\"\n    torch_dtype = torch.bfloat16\nelse:\n    attn_implementation = \"eager\"\n    torch_dtype = torch.float16","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T16:17:08.80963Z","iopub.execute_input":"2025-11-07T16:17:08.809918Z","iopub.status.idle":"2025-11-07T16:17:08.889636Z","shell.execute_reply.started":"2025-11-07T16:17:08.809896Z","shell.execute_reply":"2025-11-07T16:17:08.88897Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# QLoRA config with normal float 4\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\n# LoRA config, adding to the linear (matrix multiply) layers\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)","metadata":{"execution":{"iopub.status.busy":"2025-11-07T16:17:08.890633Z","iopub.execute_input":"2025-11-07T16:17:08.890905Z","iopub.status.idle":"2025-11-07T16:17:08.897606Z","shell.execute_reply.started":"2025-11-07T16:17:08.890884Z","shell.execute_reply":"2025-11-07T16:17:08.896862Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Instruct is already fine-tuned for instructions rather than completion.\nbase_model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model)\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=attn_implementation\n)","metadata":{"execution":{"iopub.status.busy":"2025-11-07T16:17:08.899762Z","iopub.execute_input":"2025-11-07T16:17:08.900019Z","iopub.status.idle":"2025-11-07T16:19:07.462001Z","shell.execute_reply.started":"2025-11-07T16:17:08.9Z","shell.execute_reply":"2025-11-07T16:19:07.461255Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd0d37c0b83940ccb42c38a637269ee7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"639dcbe9df0e41feaf13823e3e7d6607"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6846ffc88a85410f914e0ee2d4139785"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b4259ace6a441e0b73a02ccb0f65ffc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"521ed13a9ad745359125d481cb8c087b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd132649382e40ea958791f98a732287"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ebae69be75e4879a6ab14ca5dafb4eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"524edc53373a4fd88eb76fc61f1c47be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66a7c47837fc4a79807a428d11a07670"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2d9ecd2664c4cb98cce78fcca319654"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99bbde8607d6494a8a5b1e1e182dd909"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d037262dbcc4b218ad7e7ee7a031060"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Adding quantization and LoRA to the model\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2025-11-07T16:19:07.463036Z","iopub.execute_input":"2025-11-07T16:19:07.463294Z","iopub.status.idle":"2025-11-07T16:19:08.201491Z","shell.execute_reply.started":"2025-11-07T16:19:07.463274Z","shell.execute_reply":"2025-11-07T16:19:08.200705Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# Preparing the dataset","metadata":{}},{"cell_type":"markdown","source":"### The dataset has been shuffled, split and set to the correct format in advance.","metadata":{}},{"cell_type":"code","source":"train_path = \"/kaggle/input/student-assistance/train.jsonl\"\nval_path = \"/kaggle/input/student-assistance/test.jsonl\"\n# One split for each because they're separated in advance\ntrain_set = load_dataset(\"json\", data_files = train_path, split=\"train\")\nval_set = load_dataset(\"json\", data_files = val_path, split=\"train\")","metadata":{"execution":{"iopub.status.busy":"2025-11-07T16:19:08.202547Z","iopub.execute_input":"2025-11-07T16:19:08.202836Z","iopub.status.idle":"2025-11-07T16:19:09.445269Z","shell.execute_reply.started":"2025-11-07T16:19:08.202791Z","shell.execute_reply":"2025-11-07T16:19:09.444387Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f335630439194b61a9d6da9c7e7b4063"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f725bcc79dd4c139aed2618aafa2500"}},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"# Model training","metadata":{}},{"cell_type":"code","source":"tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})","metadata":{"execution":{"iopub.status.busy":"2025-11-07T16:19:09.44631Z","iopub.execute_input":"2025-11-07T16:19:09.446551Z","iopub.status.idle":"2025-11-07T16:19:09.456197Z","shell.execute_reply.started":"2025-11-07T16:19:09.44653Z","shell.execute_reply":"2025-11-07T16:19:09.455183Z"},"trusted":true},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Supervised finetuning configuration\nlr = 1e-5\nbatch_size = 8\nmax_seq_length = 512\nepochs = 10\nseed = 42\nrepo_name = \"llama-edu\"\n\ntraining_args = SFTConfig(\n        per_device_train_batch_size = batch_size,\n        per_device_eval_batch_size = batch_size,\n        gradient_accumulation_steps = 2,\n        eval_steps = 20,\n        warmup_steps = 20,\n        num_train_epochs = epochs,\n        learning_rate = lr,\n        fp16 = not torch.cuda.is_bf16_supported(),\n        bf16 = torch.cuda.is_bf16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = seed,\n        max_seq_length = max_seq_length,\n        dataset_num_proc = 2,\n        run_name = \"llama_finetuning\",\n        output_dir = repo_name,\n        eval_strategy = \"steps\",\n        logging_strategy = \"steps\",\n    )\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = train_set,\n    eval_dataset = val_set,\n    #packing = False, # Can make training 5x faster for short sequences.\n    args = training_args,\n    peft_config = peft_config,\n)","metadata":{"execution":{"iopub.status.busy":"2025-11-07T16:19:09.457199Z","iopub.execute_input":"2025-11-07T16:19:09.457416Z","iopub.status.idle":"2025-11-07T16:19:13.596121Z","shell.execute_reply.started":"2025-11-07T16:19:09.457399Z","shell.execute_reply":"2025-11-07T16:19:13.5953Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/310 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f63c691902342dab699d6cbe832f551"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=2):   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31e0cadf2c584feca2891151a239dd31"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# Evaluation before finetuning.\ntrainer.evaluate()","metadata":{"execution":{"iopub.status.busy":"2025-11-07T16:19:13.597411Z","iopub.execute_input":"2025-11-07T16:19:13.598258Z","iopub.status.idle":"2025-11-07T16:20:28.076659Z","shell.execute_reply.started":"2025-11-07T16:19:13.598225Z","shell.execute_reply":"2025-11-07T16:20:28.075737Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='26' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13/13 11:14]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlouay-yahyaoui\u001b[0m (\u001b[33mlouay-yahyaoui-insat\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251107_162011-58pm7fzg</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/louay-yahyaoui-insat/huggingface/runs/58pm7fzg' target=\"_blank\">llama_finetuning</a></strong> to <a href='https://wandb.ai/louay-yahyaoui-insat/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/louay-yahyaoui-insat/huggingface' target=\"_blank\">https://wandb.ai/louay-yahyaoui-insat/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/louay-yahyaoui-insat/huggingface/runs/58pm7fzg' target=\"_blank\">https://wandb.ai/louay-yahyaoui-insat/huggingface/runs/58pm7fzg</a>"},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"{'eval_loss': 2.8933908939361572,\n 'eval_model_preparation_time': 0.0127,\n 'eval_runtime': 57.4576,\n 'eval_samples_per_second': 1.74,\n 'eval_steps_per_second': 0.226}"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# Training for 10 epochs.\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2025-11-07T16:20:28.077674Z","iopub.execute_input":"2025-11-07T16:20:28.077928Z"},"trusted":true},"outputs":[{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='27' max='190' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 27/190 12:20 < 1:20:27, 0.03 it/s, Epoch 1.33/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Model Preparation Time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>3.436900</td>\n      <td>2.110035</td>\n      <td>0.012700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# Post-training evaluation\ntrainer.evaluate()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Storing the trained model in hugging face.\ntrainer.push_to_hub(\"add: llama for education\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}